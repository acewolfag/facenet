{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWLyQIgNFkb7dd3PJ6FnvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acewolfag/facenet/blob/main/FaceNet11-14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsk-4OMPh4Ae"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch albumentations opencv-python-headless torch torchvision\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!pip install tqdm\n",
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "%cd /content\n",
        "!git clone https://github.com/acewolfag/DatasetDA2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Định nghĩa các phép biến đổi tăng cường dữ liệu\n",
        "augmentation = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=15, p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
        "    A.GaussianBlur(p=0.1),\n",
        "    A.RandomGamma(p=0.2),\n",
        "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2, p=0.2),\n",
        "    A.Resize(160, 160),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Dataset tùy chỉnh với tăng cường dữ liệu\n",
        "class FaceSpoofDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, augmentations=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.augmentations = augmentations\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Tải dữ liệu từ thư mục\n",
        "        for label, folder in enumerate(['Real', 'Fake']):\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            for person in os.listdir(folder_path):\n",
        "                person_path = os.path.join(folder_path, person)\n",
        "                for img_name in os.listdir(person_path):\n",
        "                    img_path = os.path.join(person_path, img_name)\n",
        "                    self.data.append(img_path)\n",
        "                    self.labels.append(label)  # 0 = real, 1 = fake\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "        if self.augmentations:\n",
        "            image = self.augmentations(image=image)[\"image\"]\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "ZI01hfSJh9Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Khởi tạo mô hình FaceNet và classifier\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = InceptionResnetV1(pretrained='vggface2').to(device).eval()  # FaceNet\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(512, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 2)\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "01f75Tvwh-WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SIr2Trlznymd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Khởi tạo DataLoader\n",
        "dataset = FaceSpoofDataset(root_dir=\"/content/DatasetDA2/\", augmentations=augmentation)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Huấn luyện model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    # Thêm thanh tiến trình vào dataloader\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{epochs}]\", leave=False)\n",
        "\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Đảm bảo images có kiểu FloatTensor\n",
        "        images = images.float()\n",
        "\n",
        "        # Trích xuất đặc trưng từ FaceNet\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(images)\n",
        "\n",
        "        # Phân loại với classifier\n",
        "        outputs = classifier(embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Tối ưu\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Cập nhật thanh tiến trình với giá trị loss\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    # In kết quả sau mỗi epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] completed, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "J76J4L6Nh_bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(classifier.state_dict(), 'spoof_classifier.pth')\n"
      ],
      "metadata": {
        "id": "HIJzITTBiAnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations import Compose, Resize, Normalize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Định nghĩa transform để tiền xử lý ảnh\n",
        "transform = Compose([\n",
        "    Resize(160, 160),\n",
        "    Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "K2Ai2jhkpbph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải lại mô hình và lớp phân loại\n",
        "classifier.load_state_dict(torch.load('spoof_classifier.pth'))\n",
        "classifier.eval()\n",
        "\n",
        "# Hàm tiền xử lý ảnh\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = np.array(image)\n",
        "    image = transform(image=image)[\"image\"]\n",
        "    return image.float().unsqueeze(0).to(device)  # Chuyển ảnh sang FloatTensor và thêm chiều batch\n",
        "\n",
        "\n",
        "# Hàm phát hiện giả mạo\n",
        "def detect_spoof(image_path):\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embedding = model(image_tensor)\n",
        "        output = classifier(embedding)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "    if predicted.item() == 0:\n",
        "        print(\"Real person detected!\")\n",
        "    else:\n",
        "        print(\"Spoof detected!\")\n"
      ],
      "metadata": {
        "id": "Fh4tHE4Vl876"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = '/content/DatasetDA2/Real/Huy/frame_107.jpg'\n",
        "detect_spoof(test_image_path)"
      ],
      "metadata": {
        "id": "bhzObDgyl-kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n"
      ],
      "metadata": {
        "id": "teSeNILhENCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Tạo lại kiến trúc của classifier\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(512, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 2)\n",
        ")\n",
        "\n",
        "# Tải trọng số vào mô hình\n",
        "classifier.load_state_dict(torch.load('spoof_classifier.pth'))\n",
        "classifier.eval()  # Chuyển sang chế độ đánh giá\n",
        "\n",
        "# Chuyển đổi sang ONNX\n",
        "dummy_input = torch.randn(1, 512)  # Giả sử đầu vào của bạn có kích thước [1, 512]\n",
        "torch.onnx.export(classifier, dummy_input, \"spoof_classifier.onnx\",\n",
        "                  input_names=['input'], output_names=['output'],\n",
        "                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n"
      ],
      "metadata": {
        "id": "sQMJml4tl_5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "mtcnn = MTCNN(keep_all=True)\n",
        "\n",
        "# Load the PyTorch model\n",
        "model_path = '/content/spoof_classifier.pth'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.load(model_path, map_location=device)\n",
        "model.eval()\n",
        "\n",
        "# Define class labels, e.g., Real vs. Fake\n",
        "classes = ['Real', 'Fake']\n",
        "\n",
        "# JavaScript to display video stream\n",
        "def video_stream():\n",
        "    js = \"\"\"\n",
        "    const video = document.createElement('video');\n",
        "    video.setAttribute('playsinline', '');\n",
        "    video.style.display = 'block';\n",
        "    document.body.appendChild(video);\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({video: true}).then(stream => {\n",
        "      video.srcObject = stream;\n",
        "      video.play();\n",
        "    });\n",
        "\n",
        "    const canvas = document.createElement('canvas');\n",
        "    canvas.width = 640;\n",
        "    canvas.height = 480;\n",
        "    const context = canvas.getContext('2d');\n",
        "\n",
        "    function captureFrame() {\n",
        "      context.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "      return canvas.toDataURL('image/jpeg', 0.8);\n",
        "    }\n",
        "\n",
        "    async function stream() {\n",
        "      while (true) {\n",
        "        const frame = captureFrame();\n",
        "        google.colab.kernel.invokeFunction('notebook.captureFrame', [frame], {});\n",
        "        await new Promise(resolve => setTimeout(resolve, 100));\n",
        "      }\n",
        "    }\n",
        "\n",
        "    stream();\n",
        "    \"\"\"\n",
        "    display(Javascript(js))\n",
        "\n",
        "# Convert JavaScript image data to OpenCV format\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Register callback function to handle each frame\n",
        "from google.colab import output\n",
        "def capture_frame(js_reply):\n",
        "    frame = js_to_image(js_reply)\n",
        "\n",
        "    # Detect faces and get bounding boxes\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "\n",
        "    if boxes is not None:\n",
        "        for box in boxes:\n",
        "            # Extract and preprocess the face\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            face_resized = cv2.resize(face, (128, 128))  # Assuming model input size is 128x128\n",
        "            face_tensor = torch.from_numpy(face_resized).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "\n",
        "            # Run the PyTorch model for real vs. fake detection\n",
        "            with torch.no_grad():\n",
        "                pred = model(face_tensor)\n",
        "                class_id = torch.argmax(pred, dim=1).item()\n",
        "                class_name = classes[class_id]\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, f\"{class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame with overlays\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "output.register_callback('notebook.captureFrame', capture_frame)\n",
        "\n",
        "# Start video stream\n",
        "video_stream()\n"
      ],
      "metadata": {
        "id": "4exAHE0v5ASC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "c104eafa-015a-4090-ebeb-11d4db945e1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'facenet_pytorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-09c1e2f0bc08>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_hE7VQUAaMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}